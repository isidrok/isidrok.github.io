---
title: "AI Coding"
description: "Learn how to efficiently transfer data over HTTP using Node.js streams, including handling backpressure, error management, and memory optimization best practices."
date: "2025-04-12"
slug: "ai-coding"
repository: "https://github.com/isidrok/http-streams-node"
---

I have always had a thing for developer experience and productivity. My main interest has always been streamlining the development work, how can we ensure the highest quality with the lowest effort. This involves development tooling, proper architecture and abstractions and a robust working methodology.

Ever since I started working with AI coding assistants I have been trying to define a framework for optimal collaboration, both when working alone or as a team, that allows delivering quality code without having to actively worry about it.

Here I will discuss what I think is the optimal approach to coding with AI tools in order to produce high quality software.

## Table of contents

## A sensible approach to work

After using AI coding tools extensively, I have come to the conclusion that it is not that different from working with other people, it has its peculiarities but the most important point still holds:

It doesn't matter if you are working by yourself, with an AI agent or with your human peer, you cannot succeed building something if you don't know the **WHAT** and the **HOW**.
Imagine it's your first day at a new job and someone approaches you with a feature request:

```md
They> Hey buddy we need to implement a feature to detect repetitions when users are practicing kettlebell sport.
You> Oh cool, I was not aware people used kettles for training?
They> Haha, you are so damn funny, buddy. Now, lets get moving!
You> Well can you let me a hand? Is there any existing code I should look at?
They> I'm sure you will figure it out buddy, I'm pretty damn sure... 
```

Thats how many people are using AI coding tools, and the main reason why I was not successful when I first tried them out.

When approaching work you need to follow a logical approach:

1. **Ask:** What are we building? What's the starting point? What are the requirements?
2. **Plan:** How are we building this? What approaches can we follow? Which parts of the existing system need to be taken into account?
3. **Act:** Once we know **WHAT** we are doing and **HOW** we are doing it, its time to get moving.

## Collaboration

I'm also a big fan of **talking**. I usually spend lots of hours in a week talking about requirements and how to approach them from a technical perspective, constraints, drawbacks, alternatives... The time not spent here is usually dedicated to pair programming.

I love working with colleagues, specially having honest discussions with excellence driven people. We don't want a yes-man. We want honest feedback and real communication. We want our AI coding agent to:
1. Be our excellence driven, honest, pair programming peer.
2. Encourage interaction: ask questions, propose alternatives, request feedback. 

## Working with AI agents

If you have used AI coding tools for a bit you have for sure noticed a couple of things:
- They forget everything they know after each session. You keep repeating yourself over and over.
- What seems obvious to you is totally missed by them. They ignore conventions and do all kinds of weird things.
- They get distracted and as the time goes, they loose focus. The longer the session or feature, the deeper the hole.

They make up for it in plenty of ways, but in order to be efficient, we need a way for AI agents to keep its context between sessions, stay focused, and know exactly what they need to do and how.

## Memory

We will leverage the file system. In the same way we share documentation with our peers using markdown files, we can do so with our AI assistant.
Moreover, all AI coding agents come with a way to create rules, in my case I'm using `Claude Code` so those are written in `CLAUDE.md` files. Spolier: we will not have to worry about keeping this up to date.

```bash
project/
├── CLAUDE.md # Project-level AI assistant instructions
├── docs/ # Project documentation: architecture, domain, development, etc.
├── features/
│   └── feature/
│       ├── requirements.md # User stories and acceptance criteria for this feature
│       ├── plan.md         # Implementation plan and technical breakdown
│       └── progress.md     # Ongoing notes, status, and decisions for this feature
└── src/
  └── module/
    └── CLAUDE.md # Module-level AI assistant instructions
```

## Ask / Plan / Act

In order to stablish what needs to be done and how, without assumptions or confusions, and keeping our AI buddy on track, we will use three specialized prompts:

- **ask.md** will help us gather **functional** requirements, organizing them in user stories with acceptance criteria. Outputs `requirements.md`.
- **plan.md** will read those requirements and help us decide on a technical approach, then it will break down the implementation into phases, and those into components, each one with a comprehensive set of **behavioral tests**. Outputs `plan.md`.
- **act.md** will read requirements and plan, and use TDD to implement one component at a time, one test a time. Creates/updates `progress.md`.

With this approach we always know what we are doing, how we are doing it, and where we left off. Changes are incremental and there is a never ending feedback loop that lets you review changes, ask for refactors and commit code in small chunks.

I really think that the addition of TDD is what allows this framework to go an extra mile, so big shout out to Kent Beck for his [article on that matter](https://substack.com/home/post/p-166781850). 

I had been using this methodology (ask / plan / act) successfully but after adding TDD to the mix this is next level.

### Ask.md

````md
# Requirements Discovery Assistant

You are a senior requirements analyst practicing behavior-driven development. Your role is to help discover and document comprehensive requirements through targeted questions.

## Process

1. **Initial Understanding**

   - Ask clarifying questions about the feature's purpose and value
   - Identify key stakeholders and users
   - Understand the problem being solved

2. **User Story Discovery**

   - For each user type, explore their needs
   - Probe edge cases: "What happens when...?"
   - Explore failure scenarios: "How should the system behave if...?"

3. **Acceptance Criteria**
   - Transform each story into testable acceptance criteria
   - Use Given-When-Then format
   - Ensure criteria are specific and measurable

## Output Format

Create or update `features/[feature-name]/requirements.md` with:

    # [Feature Name] Requirements

    ## Overview
    [Brief description of the feature and its value]

    ## User Stories

    ### STORY-001: [Story Title]
    As a [type of user]
    I want [goal/desire]
    So that [benefit/value]

    **Acceptance Criteria:**
    1. Given [context], when [action], then [outcome]
    2. Given [context], when [action], then [outcome]

    **Edge Cases:**
    - When [edge condition], then [expected behavior]
    - When [edge condition], then [expected behavior]

    ### STORY-002: [Story Title]
    ...

    ## Non-Functional Requirements
    [List any performance, security, usability, or other cross-cutting concerns]

## Key Questions to Ask

- "Can you walk me through how a typical user would use this feature?"
- "What should happen if [common error scenario]?"
- "Are there any performance constraints I should know about?"
- "What existing systems or components will this interact with?"
- "What's the minimum viable version of this feature?"

Remember: Focus on WHAT the system should do, not HOW it should do it.
````

### Plan.md

````md
# Implementation Planning Assistant

You are a senior software architect who practices test-driven development and incremental delivery. Your role is to collaborate with the user to create actionable implementation plans that break down features into testable components.

## Collaborative Process

1. **Read Requirements**

   - Load `features/[feature-name]/requirements.md`
   - Identify all user stories and acceptance criteria
   - **Ask clarifying questions** about ambiguous requirements

2. **Component Design (Interactive)**

   - **Propose different approaches** for breaking down the feature
   - Present options: "We could structure this as A or B. A would give us X, B would give us Y."
   - **Ask for feedback** on component boundaries and responsibilities
   - Each component should have a clear public API and be implementable in 1-2 TDD cycles

3. **Test Planning (Collaborative)**

   - **Discuss testing strategy** with the user
   - For each component, collaborate on behavior-focused tests
   - **Ask**: "What's the most critical behavior to test first?"
   - Tests should cover the public API only
   - Include edge cases from requirements
   - Order tests from simplest to most complex

4. **Phase Planning (Together)**
   - **Present phasing options**: "Should we prioritize X or Y first?"
   - Group components into implementation phases
   - **Validate** that each phase delivers working functionality
   - Ensure dependencies flow naturally

**Only create the written plan after discussing and agreeing on the approach with the user.**

## Output Format

Create or update `features/[feature-name]/plan.md` with:

    # [Feature Name] Implementation Plan

    ## Architecture Overview
    [High-level design and component relationships]

    ## Phase 1: [Phase Name]

    ### Component: [Component Name]
    **Purpose**: [What this component does]
    **Stories Covered**: STORY-001, STORY-002

    **Public API**:

        // API definition in target language
        interface ComponentName {
            method(param: Type): ReturnType
        }

    **Unit Tests** (in order):
    1. test_[behavior] - [What it validates]
    2. test_[behavior] - [What it validates]
    3. test_[edge_case] - [What it validates]

    **UI Tests** (if presentation component):
    1. test_renders_[initial_state] - [Visual elements present]
    2. test_handles_[user_interaction] - [Response to user actions]
    3. test_displays_[state_changes] - [Visual feedback for state transitions]

    ### Component: [Next Component]
    ...

    ## Phase 2: [Phase Name]
    ...

    ## Integration Points
    - [Component A] connects to [Component B]: [How they interact]

    ## Risk Mitigation
    - [Identified risk]: [Mitigation strategy]

## Design Principles

- Propose alternative approaches when relevant
- Keep components small and focused
- Design for testability
- Consider performance implications early
- Make dependencies explicit

## Interaction Style

- Present design options: "We could approach this as A or B. A would give us X, B would give us Y."
- Challenge assumptions: "This assumes X. Should we also consider Y?"
- Suggest simplifications: "Could we start with a simpler version that just does X?"

````

### Act.md

````md
# TDD Implementation Assistant

You are a senior software engineer who strictly follows Test-Driven Development and Tidy First principles. Your role is to implement features incrementally, one test at a time.

## Setup

1. Load `features/[feature-name]/requirements.md` and `features/[feature-name]/plan.md`
2. Check `features/[feature-name]/progress.md` for current status
3. Find the next unmarked test in the plan

## TDD Cycle (REPEAT FOR EACH TEST)

### 1. RED Phase

- Write the next test from the plan
- Test must fail initially
- Test name should describe behavior, not implementation
- Keep test simple and focused
- Run linter/compiler to ensure no syntax errors

### 2. GREEN Phase

- Write MINIMAL code to make the test pass
- No extra functionality
- No premature optimization
- Resist adding code for future tests
- Run all tests to confirm the new one passes

### 3. REFACTOR Phase (Tidy First)

Separate structural changes from behavioral changes. Make code tidier before changing behavior.

1. Identify one improvement opportunity
2. Make the structural change
3. Run tests to ensure behavior unchanged

### 4. DOCUMENT Phase

- Add/update documentation for public APIs
- Document complex algorithms or business logic
- Ensure component purpose is clear

### 5. CHECKPOINT (When completing a component)

- Present component summary
- Show all tests passing
- Run linter and fix any issues
- Ask user: "I've completed [component name]. All tests are passing. Should I proceed to the next component?"
- Wait for user feedback even if in auto-accept mode

## Progress Tracking

Update `features/[feature-name]/progress.md`:

    # Implementation Progress

    ## Current Status
    Working on: [Component Name]
    Phase: [Current Phase]

    ## Component: [Component Name]

    ### Tests
    - [DONE] test_[behavior]
    - [IN PROGRESS] test_[behavior]
    - [TODO] test_[behavior]

    ### Notes
    [Any important decisions or observations]

## Quality Standards

- All tests must pass before moving forward
- No linter errors or warnings
- Methods should be focused and small
- Clear naming throughout
- No duplicated code

## Warning Signs

- Getting stuck on the same test repeatedly
- Adding functionality beyond current test requirements
- Disabling or modifying existing tests
- Implementing multiple behaviors at once

````

## Use case: KB Sport app
Starting from the previous post in which we studied how to run computer vision models in the browser, we will now explore how to use this methodology to complete our KB sports app.

We will start with a base project with the following structure:
    
```bash
kb-sports-app/src/
├── application/
├── domain/
├── infrastructure/
├── presentation/
└── main.tsx
```

The `application` layer will contain the use cases:
1. Start workout
2. Stop workout
3. Get workout history
4. Get workout statistics
and so on.

The `domain` layer will contain the business logic types and entities.
The `infrastructure` layer will contain the data access code
and the `presentation` layer will contain the UI code.

### Init

Create high level project documentation for claude to understand:
- project goal
- Common commands
- Source structure
- Tech stack

### ask

```md
 What feature or functionality are you looking to build or improve in your kettlebell workout tracking 
  application?
```

```
 Lets start with a basic workout feature. We want a button to start / stop the workout.\
  It must appear floating at the center bottom of the screen. Must have a glass like effect.\
  When clicking Start its label will change to Stop, we will request camera access and once the video is 
  flowing we will start analyzing poses and rendering them to the canvas. When clicking Stop, we will stop
   the camera and clear the canvas.
   ```

   10 followup questions, nothings goes under the radar, 6 additional follow up questions after that, all of them relevant.
  ```
  The requirements cover 4 main user stories:
  1. Model Loading - Preloading with status feedback
  2. Start Workout - Camera access and session initiation
  3. Stop Workout - Clean session termination
  4. UI Display - Responsive frosted glass button

  Key highlights:
  - Clear error states: Different messages for model vs camera failures
  - Loading feedback: Status popup for all async operations
  - Button state management: Proper enabling/disabling based on system state
  - Error recovery: Camera errors allow retry, model errors require page reload
  - Responsive design: Center bottom positioning across all screen sizes
  ```

  ### plan
  ```
  Based on the requirements and existing architecture, I can see we need to build a comprehensive workout
  control system. Let me discuss the implementation approach with you.
  ```


  ```
  The ML model must preload on the App component and is independent from the button.
We want the logic to reside in the /application layer and will be following ddd, so create use cases and application services to expose the functionality to the UI.

We will have:
StartWorkoutUseCase
StopWorkoutUseCase
LoadModelUseCase
(extra use cases)
and
WorkoutService


Then on the ui we will probably need two hooks and a WorkoutControlsComponent (we will be adding more at some point)
The StatusPopup is good.
All services and use cases must be exported as a singleton.
For camera errors lets tell users to fix permissions.
Por processing the poses we need to use requestAnimationFrame, ideally the ui would do that and call workoutService.processFrame, that would call the process frame use case.
```

### Learn
I'll analyze the conversation to extract and document the architectural decisions and patterns for this
  kettlebell workout tracking application.

  Spent 40 minutes, well worth it.

### Notes

- Slow to start but great architecutre, 100% coverage
- If you forget a test, its not going to be done, for example the UI
- Boring as hell
- Slow is Smooth and smooth is fast
- commit often. Component done? commit. Refactor? commit

## Preview

USE THE BEST MODEL YOU CAN FOR REQUIREMENTS AND PLANNING

9:50 /ask lets implement preview-mode
9:58 commit 5 user stories
9:58 /clear and /plan lets work on preview-mode
Separate preview service, extend workout service with flag, shared camera servie. Also presents some options for the UI.
> I like option C. Lets just create a singleHook, usePreview but lets have three components, one for each 
  button and a controls component that renders them all
How do we handle camera and pose. how do we arrange hooks, important architectural decisions.

> It didnt occur to him that 
Workout service is the one setting canvas and video dimensions and exposing the process frame method. We need to extract that to a service, lets call it PoseService, that takes care of camera, detection and pose rendering, then we can use it in our PreviewService and our WorkoutService.
This will work much better
- You need to be vigilant

10:14 /clear and /act lets work on preview-mode

Auto mode until first component completed

I've completed the PoseService component. All tests are passing. Should I proceed to the next component
  (refactoring WorkoutService to use PoseService)?

Review diff in vscode
Working? Commit
Refactor needed? Commit then ask to refactor

│ > Commit and then refactor: when clearing the canvas use the renderer adapter. Dont do it manually          │

After refactor > Commit and next
Repeat until done

Finished around 11:06

QA and bugs until 11:20

## Rep tracking 
12:00

We want to automatically count repetitions of overhead lifts such as jerk or snatch. This will be used by people training at home on their own. Normally people record themselves and count reps after a ten minutes set.
We will use the existing pose detection system to detect when the wrist has gone over the nose. We need to be very careful to not count double reps so we need a state machine:
down for 300ms + up for 300ms = rep, then back down.
Also reps can be made with left, right or both arms (always count as one) It is CRITICAL, that we first check for both arms and then for individual, to avoid double counting, and it will probably be good to have some kind of debounce. We need 100% accuracy.
We will be detecting them in real time.
For now we will just count them and display them in the ui, in a floating card with the same glass effect as the rest of the UI. System should NOT MISS or count incorrectly.

12:09 plan

We will have a RepDetectionService in the domain, with a RepDetectionState machine that encapsulates that logic.
Process frame use case will return the result and workout service will call DetectRepUseCase that will make use of the repdetectionservice.

For ui lets create a workout-stats component, with a workout-stats-card and do it that way.
We will store the reps in the workoutEntity as an array, each rep will have hand (left, right, both) and timestamp.
The stats use case needs to return the number of reps.
Lets focus on the domain first.

12:25 act

13:41